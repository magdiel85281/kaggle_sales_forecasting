{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sales_data import SalesData\n",
    "# import nlp as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir('../data'):\n",
    "    if '.csv' in filename:\n",
    "        df = pd.read_csv(f'../data/{filename}')\n",
    "        print(f'{filename}:\\t{df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data and Format Columns\n",
    "Also, add year, month, and year_month columns. Then, aggregate to monthly sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SalesData()\n",
    "sd.set_sales_data()\n",
    "\n",
    "sd.merge_shop_data_to_sales()\n",
    "sd.merge_item_data_to_sales()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.monthly_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.monthly_sales.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get unique shop/items\n",
    "... and get dummies for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_items = sd.monthly_sales.loc[:, ['shop_id', \n",
    "                                      'item_id',  \n",
    "                                      'loc_name', \n",
    "                                      'тц', \n",
    "                                      'трц',\n",
    "                                      'мега', \n",
    "                                      'тк', \n",
    "                                      'трк', \n",
    "                                      'молл', \n",
    "                                      'центральный', \n",
    "                                      'item_category_name']].copy().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data=shop_items, \n",
    "                      prefix='loc_name', \n",
    "                      prefix_sep='_', \n",
    "                      columns=['loc_name'], \n",
    "                      drop_first=True)\n",
    "\n",
    "data = pd.get_dummies(data=data, \n",
    "                      prefix='cat', \n",
    "                      prefix_sep='_', \n",
    "                      columns=['item_category_name'], \n",
    "                      drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Break out monthly sales data\n",
    "Create crosstab of item counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sd.monthly_sales.loc[:, ['year_month', \n",
    "                                 'month', \n",
    "                                 'shop_id',\n",
    "                                 'item_id',\n",
    "                                 'item_cnt_month']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_ct = pd.crosstab(index=[sales['year_month'], sales['shop_id'], sales['item_id']], \n",
    "                    columns=sales.loc[:, 'month'], \n",
    "                    values=sales.loc[:, 'item_cnt_month'],\n",
    "                      aggfunc='mean').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge it all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.merge(data, sales_ct, on=['shop_id', 'item_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in model_data.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models\n",
    "Get a baseline score using Gradient Boosting and Random Forest Regressors. First run used GridSearch to find best parameters... probably overkill for our baseline models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_split(X, y, test_periods=1):\n",
    "    periods = sorted(list(X['year_month'].unique()))\n",
    "    train_periods = periods[:-test_periods]\n",
    "    \n",
    "    train_mask = X['year_month'].isin(train_periods)\n",
    "    X_train = X.loc[train_mask].copy()\n",
    "    y_train = y.loc[train_mask].copy()\n",
    "    \n",
    "    X_test = X.loc[~train_mask].copy()\n",
    "    y_test = y.loc[~train_mask].copy()\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_cols = model_data.columns[: -3]\n",
    "X = model_data.loc[:, feat_cols].copy()\n",
    "y = model_data.loc[:, '10'].copy()\n",
    "\n",
    "X.fillna(0, inplace=True)\n",
    "y.fillna(0, inplace=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = ts_train_test_split(X, y, test_periods=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop year_month and year\n",
    "for df in [X_train, X_test]:\n",
    "    df.drop(['year_month' \n",
    "#              'year',\n",
    "#              'avg_price'\n",
    "            ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "sc_y = StandardScaler()\n",
    "y_train = sc_y.fit_transform(np.array(y_train).reshape(1, -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestRegressor(n_jobs=1)\n",
    "# params = {'n_estimators': [100, 500, 1000], 'max_depth': [2, 3]}\n",
    "# gs = GridSearchCV(estimator=rf, param_grid=params, verbose=3)\n",
    "# gs.fit(X_train, y_train.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for k in gs.cv_results_.keys():\n",
    "#     print(f'{k}:\\n{gs.cv_results_[k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500, \n",
    "                           criterion='mse', \n",
    "                           max_depth=2, \n",
    "                           min_samples_split=2, \n",
    "                           min_samples_leaf=1, \n",
    "                           min_weight_fraction_leaf=0.0, \n",
    "                           max_features='auto', \n",
    "                           max_leaf_nodes=None, \n",
    "                           min_impurity_decrease=0.0, \n",
    "                           min_impurity_split=None, \n",
    "                           bootstrap=True, \n",
    "                           oob_score=False, \n",
    "                           n_jobs=2, \n",
    "                           random_state=123, \n",
    "                           verbose=1, \n",
    "                           warm_start=False, \n",
    "                           ccp_alpha=0.0, \n",
    "                           max_samples=None)\n",
    "\n",
    "rf.fit(X_train, y_train.ravel())\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Random Forest Regressor RMSE: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb = GradientBoostingRegressor(criterion='mse', n_iter_no_change=100)\n",
    "# params = {'n_estimators': [100, 200, 400], \n",
    "#           'max_depth': [2, 3, 4], \n",
    "#           'learning_rate': [0.05, 0.1]}\n",
    "# gs = GridSearchCV(estimator=gb, param_grid=params, n_jobs=1, verbose=3)\n",
    "# gs.fit(X_train, y_train.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for k in gs.cv_results_.keys():\n",
    "#     print(f'{k}:\\n{gs.cv_results_[k]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_score_idx = np.argmin(gs.cv_results_['rank_test_score'])\n",
    "# gs.cv_results_['params'][best_score_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(loss='ls', \n",
    "                               learning_rate=0.1, \n",
    "                               n_estimators=200, \n",
    "                               subsample=1.0, \n",
    "                               criterion='mse', \n",
    "                               min_samples_split=2, \n",
    "                               min_samples_leaf=1, \n",
    "                               min_weight_fraction_leaf=0.0, \n",
    "                               max_depth=2, \n",
    "                               min_impurity_decrease=0.0, \n",
    "                               min_impurity_split=None, \n",
    "                               init=None, \n",
    "                               random_state=None, \n",
    "                               max_features=None, \n",
    "                               alpha=0.9, \n",
    "                               verbose=1, \n",
    "                               max_leaf_nodes=None, \n",
    "                               warm_start=False, \n",
    "                               presort='deprecated', \n",
    "                               validation_fraction=0.1, \n",
    "                               n_iter_no_change=100, \n",
    "                               tol=0.0001, \n",
    "                               ccp_alpha=0.0)\n",
    "\n",
    "gb.fit(X_train, y_train.ravel())\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Gradient Boosting Regressor RMSE: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Output\n",
    "Import the test set and get results to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('data/test.csv')\n",
    "test_set.insert(loc=1, column='month', value='11')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_out = gb.predict(test_set.iloc[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'ID': np.arange(0, test_set.shape[0]), \n",
    "    'item_cnt_month': y_pred_out\n",
    "}\n",
    "output = pd.DataFrame(data=d)\n",
    "output.to_csv('output/submission_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical info in shops, potentially\n",
    "A quick glance into the translations of some of the shop_names in shops indicated that I might be able to break out some categorical info. The first word might be a city or some other location. Also, some of the words which have higher frequencies seem to point to either a shopping center, a mall, a megastore, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up names, get locations, then vectorize the top occurences\n",
    "shops = sd.shops\n",
    "shops['clean_name'] = nlp.clean_names(shops['shop_name'])\n",
    "\n",
    "shops['loc_name'] = shops['clean_name'].apply(lambda x: x.split()[0])\n",
    "shops = nlp.get_top_words(shops, shops['clean_name'], 10)\n",
    "\n",
    "# remove top occurences if in loc_name\n",
    "for col in shops.columns:\n",
    "    if col.upper() in shops['loc_name'].unique():\n",
    "        shops.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with Additional Shop Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shops['shop_id'] = shops['shop_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sales = pd.merge(sales, shops, on='shop_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['shop_name', 'clean_name']\n",
    "for col in drop_cols:\n",
    "    shop_sales.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_cols = ['year', 'month', 'year_month', 'shop_id', \n",
    "           'item_id', 'loc_name', \n",
    "           'тц', 'трц', 'мега', 'тк', 'трк', 'молл', 'центральный']\n",
    "\n",
    "model_input = shop_sales.groupby(gb_cols)[['item_cnt_mth']].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = pd.get_dummies(data=model_input, prefix='loc', prefix_sep='_', \n",
    "                             columns=['loc_name'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_input.drop('item_cnt_mth', axis=1)\n",
    "y = model_input.loc[:, 'item_cnt_mth'].copy()\n",
    "X_train, X_test, y_train, y_test = ts_train_test_split(X, y, test_periods=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [X_train, X_test]:\n",
    "    df.drop(['year', 'year_month'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=500, \n",
    "                           criterion='mse', \n",
    "                           max_depth=2, \n",
    "                           min_samples_split=2, \n",
    "                           min_samples_leaf=1, \n",
    "                           min_weight_fraction_leaf=0.0, \n",
    "                           max_features='auto', \n",
    "                           max_leaf_nodes=None, \n",
    "                           min_impurity_decrease=0.0, \n",
    "                           min_impurity_split=None, \n",
    "                           bootstrap=True, \n",
    "                           oob_score=False, \n",
    "                           n_jobs=2, \n",
    "                           random_state=123, \n",
    "                           verbose=1, \n",
    "                           warm_start=False, \n",
    "                           ccp_alpha=0.0, \n",
    "                           max_samples=None)\n",
    "\n",
    "rf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Random Forest Regressor RMSE: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingRegressor(loss='ls', \n",
    "                               learning_rate=0.1, \n",
    "                               n_estimators=200, \n",
    "                               subsample=1.0, \n",
    "                               criterion='mse', \n",
    "                               min_samples_split=2, \n",
    "                               min_samples_leaf=1, \n",
    "                               min_weight_fraction_leaf=0.0, \n",
    "                               max_depth=2, \n",
    "                               min_impurity_decrease=0.0, \n",
    "                               min_impurity_split=None, \n",
    "                               init=None, \n",
    "                               random_state=None, \n",
    "                               max_features=None, \n",
    "                               alpha=0.9, \n",
    "                               verbose=1, \n",
    "                               max_leaf_nodes=None, \n",
    "                               warm_start=False, \n",
    "                               presort='deprecated', \n",
    "                               validation_fraction=0.1, \n",
    "                               n_iter_no_change=100, \n",
    "                               tol=0.0001, \n",
    "                               ccp_alpha=0.0)\n",
    "\n",
    "gb.fit(X_train, y_train.values.ravel())\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'Gradient Boosting Regressor RMSE: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Output\n",
    "Import the test set and get results to upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('data/test.csv')\n",
    "test_set.insert(loc=1, column='month', value='11')\n",
    "test_set['shop_id'] = test_set['shop_id'].astype(str)\n",
    "print(test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.merge(test_set, shops, on='shop_id', how='inner')\n",
    "\n",
    "test_set = pd.get_dummies(data=test_set, prefix='loc', prefix_sep='_', \n",
    "                             columns=['loc_name'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add missing columns\n",
    "for col in X_train.columns:\n",
    "    if col not in test_set.columns:\n",
    "        print(f'Adding columns {col}')\n",
    "        test_set[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set.loc[:, X_train.columns]\n",
    "y_pred_out = gb.predict(test_set)\n",
    "\n",
    "y_pred_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'ID': np.arange(0, test_set.shape[0]), \n",
    "    'item_cnt_month': y_pred_out\n",
    "}\n",
    "output = pd.DataFrame(data=d)\n",
    "output.to_csv('output/submission_gb_shop_categories.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Insights\n",
    "Decision Tree models not really doing the job we need it to here. The RMSE scores are terrible. Time Series modeling is likely the better approach. Let's see what diffferent aggregations look like. Then, maybe we can pick a particular shop-item combination to use as a template for our TS model..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
